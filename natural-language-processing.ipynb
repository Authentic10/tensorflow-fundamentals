{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef1fdac-91e0-4e7b-a987-3b818b5c48b4",
   "metadata": {},
   "source": [
    "# Representating text as Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aa651b-a51c-4573-8542-5350279799ab",
   "metadata": {},
   "source": [
    "To solve NLP tasks, we need to represent text as tensors. We can use different approaches when representing text :\n",
    "- Character level representation, where we represent text by treating each character as a number. \n",
    "- Word-level representation, in which we create a vocabulary of all words in our text, and then represent words using one-hot-encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83197fd0-f989-4e73-bdeb-251643e87dab",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d455d4cd-7fd4-4f7d-8021-110f042e6e36",
   "metadata": {},
   "source": [
    "We'll start with a text classification task based on the AG_NEWS dataset. We'll classify news headlines into one of 4 categories: World, Sports, Business and Sci/Tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca128ae3-e874-4a50-8df7-f8d98f1a9200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d62b91-f197-4673-a8b8-ac0d69b91662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Ahmed\\tensorflow_datasets\\ag_news_subset\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fff2fd118d454297a669e2ab9b172e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427a7ef3781d4289ad475d5080b176f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bb16550b6d4f91b07f4954bf7671d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ag_news_subset-train.tfrecord...:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ag_news_subset-test.tfrecord...:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset ag_news_subset downloaded and prepared to C:\\Users\\Ahmed\\tensorflow_datasets\\ag_news_subset\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = tfds.load('ag_news_subset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b6453db-2509-4f56-b35c-3848ca25d26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset = 120000\n",
      "Length of test dataset = 7600\n"
     ]
    }
   ],
   "source": [
    "ds_train = dataset['train']\n",
    "ds_test = dataset['test']\n",
    "\n",
    "print(f\"Length of train dataset = {len(ds_train)}\")\n",
    "print(f\"Length of test dataset = {len(ds_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a48b4ada-99ef-4593-afe9-dffc19824504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (Sci/Tech) -> b'AMD Debuts Dual-Core Opteron Processor' b'AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.'\n",
      "1 (Sports) -> b\"Wood's Suspension Upheld (Reuters)\" b'Reuters - Major League Baseball\\\\Monday announced a decision on the appeal filed by Chicago Cubs\\\\pitcher Kerry Wood regarding a suspension stemming from an\\\\incident earlier this season.'\n",
      "2 (Business) -> b'Bush reform may have blue states seeing red' b'President Bush #39;s  quot;revenue-neutral quot; tax reform needs losers to balance its winners, and people claiming the federal deduction for state and local taxes may be in administration planners #39; sights, news reports say.'\n",
      "3 (Sci/Tech) -> b\"'Halt science decline in schools'\" b'Britain will run out of leading scientists unless science education is improved, says Professor Colin Pillinger.'\n",
      "1 (Sports) -> b'Gerrard leaves practice' b'London, England (Sports Network) - England midfielder Steven Gerrard injured his groin late in Thursday #39;s training session, but is hopeful he will be ready for Saturday #39;s World Cup qualifier against Austria.'\n"
     ]
    }
   ],
   "source": [
    "classes = ['World','Sports','Business','Sci/Tech']\n",
    "\n",
    "for i,x in zip(range(5), ds_train):\n",
    "    print(f\"{x['label']} ({classes[x['label']]}) -> {x['title']} {x['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12c3b91-da35-425a-a40e-332aef5b9140",
   "metadata": {},
   "source": [
    "### Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfac421-1e55-41e7-b3fb-c9f315dadcf8",
   "metadata": {},
   "source": [
    "We need to convert text into numbers that can be represented as tensors. For word-level representation, we need two things :\n",
    "- Use a tokenizer to split texts into tokens\n",
    "- Build a vocabulary of those tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f4bddf-b56a-4711-a44c-ac2a0ab31645",
   "metadata": {},
   "source": [
    "The vocabulary size is big in the dataset (more than 100k words). Generally speaking, we don't need words that are rarely present in the text so we are going to limit the vocabulary size to a smaller number by passing an argument to the vectorizer constructor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9cf59de-fe96-4cd4-ac6b-edf3a61fb586",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size)\n",
    "vectorizer.adapt(ds_train.take(500).map(lambda x: x['title']+' '+x['description']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5575d904-48bb-4ba5-a4ce-5c109dce0828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'to', 'a', 'in', 'of', 'and', 'on', 'for']\n",
      "Length of vocabulary : 5335\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_vocabulary()\n",
    "vocab_size = len(vocab)\n",
    "print(vocab[:10])\n",
    "print(f\"Length of vocabulary : {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d16a0e-6ad7-4bf0-b8d8-7d9bf5a81c8c",
   "metadata": {},
   "source": [
    "### Bag-of-words text representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28cafd4-25cc-4bb6-864d-bd5a6ae2a1c2",
   "metadata": {},
   "source": [
    "In Bag-of-Words(BoW) vector representation, each word is linked to a vector index, and a vector element contains the number of occurences of each word in a given document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fe3a265-baf7-441d-a205-db9ecb7e8799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 2, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sc_vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "    'I like hot dogs.',\n",
    "    'The dog ran fast',\n",
    "    'Its hot outside',\n",
    "]\n",
    "sc_vectorizer.fit_transform(corpus)\n",
    "sc_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8e403f-52e7-4b63-af84-57b27ce3a122",
   "metadata": {},
   "source": [
    "We can also use the Keras that we defined above, converting each word number into a one-hot-encoding and adding all those vectors up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a00ffddf-5fd7-4464-84a6-b5284a87f4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_bow(text):\n",
    "    return tf.reduce_sum(tf.one_hot(vectorizer(text), vocab_size), axis=0)\n",
    "\n",
    "to_bow('My dog likes hot dogs on a hot day.').numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b2784-724b-429c-87c0-777bbdec1158",
   "metadata": {},
   "source": [
    "### Training the BoW classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4532c83-dbca-48fc-8b18-c40a7d1d855e",
   "metadata": {},
   "source": [
    "Let's train a classifier that uses the BoW representation. First, we need to convert our dataset to a bag-of-words representation. This can be achieved by using map function in the following way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5fd87fc-c447-4a4d-b20c-2511db4c5b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "ds_train_bow = ds_train.map(lambda x: (to_bow(x['title']+x['description']),x['label'])).batch(batch_size)\n",
    "ds_test_bow = ds_test.map(lambda x: (to_bow(x['title']+x['description']),x['label'])).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaf0ea1-494c-47b6-ac74-6671620ed931",
   "metadata": {},
   "source": [
    "Now, let's define a simple classifier neural network that contains one linear layer. The input size is vocab_size, and the output size corresponds to the number of classes (4). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cf9ef17-c7a7-4f39-9727-9ea070c6fd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 96s 102ms/step - loss: 0.6157 - acc: 0.8425 - val_loss: 0.4416 - val_acc: 0.8716.626 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x215bb43a7c0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(4, activation='softmax', input_shape=(vocab_size,))\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(ds_train_bow, validation_data=(ds_test_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b306dc58-b149-4a0c-8209-00bbadbe3d67",
   "metadata": {},
   "source": [
    "### Training a classifier as one network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84679d86-8b8a-49be-a1b0-042e33a1dd30",
   "metadata": {},
   "source": [
    "Because the vectorizer is also a Keras layer, we can define a network that includes it, and train it end-to-end. This way we don't need to vectorize the dataset using map, we can just pass the original dataset to the input of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e031b1-9c23-44e9-b428-8ab5147d126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(x):\n",
    "    return x['title']+' '+x['description']\n",
    "\n",
    "def tupelize(x):\n",
    "    return (extract_text(x),x['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b3cb6dd-87e4-4fec-a53b-62bbb235ecfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization_4 (TextVe (None, None)              0         \n",
      "_________________________________________________________________\n",
      "tf.one_hot_3 (TFOpLambda)    (None, None, 5335)        0         \n",
      "_________________________________________________________________\n",
      "tf.math.reduce_sum_3 (TFOpLa (None, 5335)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 21344     \n",
      "=================================================================\n",
      "Total params: 21,344\n",
      "Trainable params: 21,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "938/938 [==============================] - 27s 28ms/step - loss: 0.6012 - acc: 0.8447 - val_loss: 0.4199 - val_acc: 0.8734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x214d18bfe80>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = keras.Input(shape=(1,), dtype=tf.string)\n",
    "x = vectorizer(inp)\n",
    "x = tf.reduce_sum(tf.one_hot(x, vocab_size), axis=1)\n",
    "out = keras.layers.Dense(4, activation='softmax')(x)\n",
    "model = keras.models.Model(inp, out)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size), validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abcfd1e-ab51-4859-813f-a25c93d2d315",
   "metadata": {},
   "source": [
    "### Bigrams, trigrams and n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d8dcea-1d2b-4ee6-9398-8285c16e7140",
   "metadata": {},
   "source": [
    "One limitation of the bag-of-words approach is that some words are part of multi-word expressions, for example, the word 'hot dog' has a completely different meaning from the words 'hot' and 'dog' in other contexts. If we represent the words 'hot' and 'dog' always using the same vectors, it can confuse our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ca29d-cdd2-434d-97cc-ac99f0cc5e30",
   "metadata": {},
   "source": [
    "To address this, n-grams representation are often used in methods of document classification, where the frequency of each word, bi-word or tri-word is a useful feature for training classifiers. In bigram representations, for example, we will add all word in pairs to the vocabulary, in addition to original words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12205246-ec1d-4784-bbb2-fd245a62a454",
   "metadata": {},
   "source": [
    "Example of the genration of a bigram bag of word representation with Scikit Learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "235e8ec2-bf32-4078-8ca5-696780a046f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " {'i': 7, 'like': 11, 'hot': 4, 'dogs': 2, 'i like': 8, 'like hot': 12, 'hot dogs': 5, 'the': 16, 'dog': 0, 'ran': 14, 'fast': 3, 'the dog': 17, 'dog ran': 1, 'ran fast': 15, 'its': 9, 'outside': 13, 'its hot': 10, 'hot outside': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1,2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "bigram_vectorizer.fit_transform(corpus)\n",
    "print(\"Vocabulary:\\n\",bigram_vectorizer.vocabulary_)\n",
    "bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a63b377-2a63-49e0-8791-f72a432e0692",
   "metadata": {},
   "source": [
    "The main drawback of the n-gram approach is that the vocabulary size starts to grow extremely fast. In practice, we need to combine the n-gram representation with a dimensionality reduction technique, such as embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b079487b-fadf-4614-8d15-6661dc89f4f7",
   "metadata": {},
   "source": [
    "### Automatically calculating BoW Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05918c5-1a4f-43b3-9f01-43a14f1c817e",
   "metadata": {},
   "source": [
    "In the example above, we calculated BoW vectors by hand by summing the one-hot-encodings of individual words. However, the latest version of Tensorflow allows us to calculate BoW vectors automatically by passing the `output_mode='count'` parameter to the vectorizer constructor. This makes defining and training our model significantly easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3dd6da4c-f8fd-4878-bb44-61d1eee3a323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 33s 34ms/step - loss: 0.5918 - acc: 0.8499 - val_loss: 0.4162 - val_acc: 0.8764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x215bcf4cb80>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size, output_mode='count'),\n",
    "    keras.layers.Dense(4, input_shape=(vocab_size, ), activation='softmax')\n",
    "])\n",
    "print(\"Training vectorizer\")\n",
    "model.layers[0].adapt(ds_train.take(500).map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size), validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5245f006-b80c-41a8-ae34-60992133dfa6",
   "metadata": {},
   "source": [
    "### Term Frequency - inverse document frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803c954b-959b-469d-ac57-690ff86d9da3",
   "metadata": {},
   "source": [
    "In Bow representation, word occurences are weighted using the same technique regardless of the word itself. However, it's clear that frequent words such as *a* and *in* are much less important for classification than specialized terms. In most NLP tasks some words are more relevant than others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de46b96-74d1-49ab-9841-d45cd59b811c",
   "metadata": {},
   "source": [
    "TF-IDF is a variation of bag-of-words, where instead of a binary 0/1 value indicating the appearence of a word in a document, a floating-point value is used, which is related to the frequency of the word occurence in the corpus.\n",
    "The TF-IDF value increases proportionally to the number of times a word appears in a document and is offset by the number of documents in the corpus that contains the word, which helps to adjust for the fact that some words appear more frequently than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "626c793e-a578-402b-b451-638d7bcc850b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43381609, 0.        , 0.43381609, 0.        , 0.65985664,\n",
       "        0.43381609, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55660b10-89e2-42c6-91ab-645f1f6955c4",
   "metadata": {},
   "source": [
    "In Keras, the TextVectorization layer can automatically compute TF-IDF frequencies by passing the `output_mode='tf-idf'` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d84d91d6-ce4a-41e3-bcee-db0a450943f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 30s 31ms/step - loss: 0.3998 - acc: 0.8656 - val_loss: 0.3496 - val_acc: 0.8828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x215bb737a30>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size, output_mode='tf-idf'),\n",
    "    keras.layers.Dense(4, input_shape=(vocab_size, ), activation='softmax')\n",
    "])\n",
    "print(\"Training vectorizer\")\n",
    "model.layers[0].adapt(ds_train.take(500).map(extract_text))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size), validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3054bd2-2fb6-4186-94d8-96677f2fd408",
   "metadata": {},
   "source": [
    "Even though TF-IDF representations provide frequency weights to different words, they are unable to represent meaning or order. We will learn how to capture contextual information from text using language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf20b08-d681-4318-9428-a7761971335a",
   "metadata": {},
   "source": [
    "# Represent words with embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd7f4fa-dc98-4a34-8a3f-d6fcfa9d7913",
   "metadata": {},
   "source": [
    "The idea of embedding is to represent words using lower-dimensional dense vectors that reflect the semantic meaning of the word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "241d6c12-09cd-40c1-8f9f-d2f417eba209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization_8 (TextVe (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 100)         3000000   \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 3,000,404\n",
      "Trainable params: 3,000,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "batch_size = 128\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size, input_shape=(1,))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,\n",
    "    # Embedding layer takes n numbers and reduces each number to a dense vector\n",
    "    keras.layers.Embedding(vocab_size, 100),\n",
    "    # Aggregation layer computes the average of all n input tensors corresponding to different words\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1)),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7f9e8c2-e2f2-425c-8e8a-686a2c392a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "938/938 [==============================] - 20s 19ms/step - loss: 0.7901 - acc: 0.8129 - val_loss: 0.4510 - val_acc: 0.8637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x215bb535940>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training vectorizer\")\n",
    "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size), validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24907048-e322-468b-b621-55ff1914d31a",
   "metadata": {},
   "source": [
    "## Semantic embeddings: Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb3dd57-7c13-460f-b2e8-23489b340f0e",
   "metadata": {},
   "source": [
    "In the previous example, the embedding layer learned to map words to vector representations, however, these representations did not have semantic meaning. To learn a vector representation in which similar words or synonyms corresponds to vectors that are close to each other in terms of some vector distance, we need to pretrain our embedding model on a large collection of text using a technique such as Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63791fe2-e36d-4beb-b84c-b4435cc82203",
   "metadata": {},
   "source": [
    "Word2Vec is based on two main architectures that are used to produce a distributed representation of words :\n",
    "- Continuous bag-of-words (CBoW), where we train the model to predict a word from the surrounding context\n",
    "- Continuous skip-gram is opposite to CBoW. The model uses the surrounding window of context words to predict the current word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111c065d-97cc-46b6-b57a-03a0bb0eb880",
   "metadata": {},
   "source": [
    "To experiment with the Word2Vec embedding pretrained on Google News dataset, we can use the **gensim** library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c49519f-be0e-4e91-9ba3-fba1981f6eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be91f59f-779b-4c35-b438-e3634f015208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuronal -> 0.780479907989502\n",
      "neurons -> 0.7326499223709106\n",
      "neural_circuits -> 0.7252850532531738\n",
      "neuron -> 0.7174385190010071\n",
      "cortical -> 0.6941086649894714\n",
      "brain_circuitry -> 0.6923245787620544\n",
      "synaptic -> 0.6699119210243225\n",
      "neural_circuitry -> 0.6638563275337219\n",
      "neurochemical -> 0.6555314660072327\n",
      "neuronal_activity -> 0.6531826853752136\n"
     ]
    }
   ],
   "source": [
    "for w,p in w2v.most_similar('neural'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e5cd3b-1925-4bfe-a80e-f3bff878f225",
   "metadata": {},
   "source": [
    "With Semantic embeddings, we can manipulate the vector encoding based on semantics. For example, we can ask to find a word whose vector representation is as close as possible to the words king and woman, and as far as possible from the word man. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4eaae822-3478-497d-a647-1d52a10a4b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118193507194519)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king', 'woman'], negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43d73ea-152e-4ac0-8837-6e49158523a5",
   "metadata": {},
   "source": [
    "Word2Vec seems like a great way to express word semantics but it has many disadvantages, including the following:\n",
    "- Both CBoW and skip-gram models are predictive embeddings, and they only take local context into account. \n",
    "Word2Vec does not take advantage of global context\n",
    "- Word2Vec does not take into account word morphology, i.e the fact that the meaning of the word can depend on different parts of the word such as the root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b32c331-91fd-40a7-bd98-a803c7b34f07",
   "metadata": {},
   "source": [
    "**FastText** tries to overcome the second limitation, and builds on Word2Vec by learning vector representation for each word and the character n-grams found within each word. The values of the representations are then averaged into one vector at each training step. While this adds a lot of additional computation to pretraining, it enables word embeddings to encode sub-word information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273ac10-911a-4879-bd29-1523a6a54b49",
   "metadata": {},
   "source": [
    "Another method, **GloVe** uses a different approach to word embeddings, based on the factorization of the word-context matrix. First, it builds a large matrix that counts the number of word occurences in different contexts, and then it tries to represent this matrix in lower dimensions in a way that minimizes reconstruction loss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bbf68f-8131-4854-a680-77d316256264",
   "metadata": {},
   "source": [
    "## Using pretrained embeddings in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8683ff-a0be-49fb-9de1-2cef33ba8b96",
   "metadata": {},
   "source": [
    "There are two possible options: tokenizer vocabulary and vocabulary from Word2Vec embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3178d9fa-6891-4262-a3a5-92c9abdf19ab",
   "metadata": {},
   "source": [
    "### Tokenizer Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5928ca96-c62d-42d5-bb8f-e279c46a6be0",
   "metadata": {},
   "source": [
    "In this method, some of the words from the vocabulary will have corresponding Word2Vec embeddings, and some will be missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e17f0cf3-f2b3-4ca4-9d03-bd1855ec3110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 300\n",
      "Populating matrix, this will take some time...Done, found 4551 words, 784 words missing\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f\"Embedding size: {embed_size}\")\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "W = np.zeros((vocab_size, embed_size))\n",
    "print('Populating matrix, this will take some time...', end='')\n",
    "found, not_found = 0, 0\n",
    "for i, w in enumerate(vocab):\n",
    "    try:\n",
    "        W[i] = w2v.get_vector(w)\n",
    "        found += 1\n",
    "    except:\n",
    "        not_found += 1\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9e6be0-236b-441a-bdcb-1fb0539731dc",
   "metadata": {},
   "source": [
    "For words that are not present in the Word2Vec vocabulary, we can either leave them as zeroes, or generate a random vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4b4e37c-2fd0-4dd8-8922-005c84f3c8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = keras.layers.Embedding(vocab_size, embed_size, weights=[W], trainable=False) \n",
    "# Trainable=False will not retrain the embeddings\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer, emb,\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f653bfef-f074-47c4-83ff-bdc274ea49fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 25s 25ms/step - loss: 1.1050 - acc: 0.7930 - val_loss: 0.9120 - val_acc: 0.8180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x216c7d2a1f0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size), \n",
    "          validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7555a496-c19c-492c-94ec-4433cc0b3d78",
   "metadata": {},
   "source": [
    "### Embeddings vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6526bfe2-459d-4e5f-a833-2c5932d5808d",
   "metadata": {},
   "source": [
    "One issue with the previous approach is that the vocabularies used in the TextVectorization and Embedding are different. To overcome this problem, we can use one of the following solutions :\n",
    "- Retrain Word2Vec model on our vocabulary\n",
    "- Load our dataset with the vocabulary from the pretrained Word2Vec model. Vocabularies used to load the dataset can be specified during loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9c45a2a6-f017-432f-ba25-2aece8e1c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.index_to_key)\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
    "vectorizer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad1741f-c490-4936-a99c-0731ed4691b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"model = keras.models.Sequential([\n",
    "    vectorizer,\n",
    "    w2v.wv.get_keras_embedding(train_embeddings=False),\n",
    "    keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "model.fit(ds_train.map(tupelize).batch(128),\n",
    "          validation_data=ds_test.map(tupelize).batch(128), epochs=5)\"\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60153c52-f82a-4697-8bb7-5991f44a7e65",
   "metadata": {},
   "source": [
    "### Contextual Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0678ca-a851-43b5-9e36-ccb1923a7310",
   "metadata": {},
   "source": [
    "One key limitation of traditional pretrained embedding representations such as Word2Vec is the fact that, even though they can capture some meaning of a word, they can't differentiate between different meanings. This can cause problems in downstream models.\n",
    "\n",
    "For example the word 'play' has different meaning in these two different sentences:\n",
    "- I went to a **play** at the theater.\n",
    "- John wants to **play** with his friends.\n",
    "\n",
    "The pretrained embeddings we talked about represent both meanings of the word 'play' in the same embedding. To overcome this limitation, we need to build embeddings based on the **language model**, which is trained on a large corpus of text, and *knows* how words can be put together in different contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3442ef9f-7ce6-4bae-94be-a464ecdee0cd",
   "metadata": {},
   "source": [
    "# Capture patterns with recurrent neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e0c8ec-db7a-43fa-ba39-2f3643600ee2",
   "metadata": {},
   "source": [
    "## Recurrent neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34625ce-30b6-4ba0-8d92-d820220d1b75",
   "metadata": {},
   "source": [
    "The models we've benn using so far are unable to represent word orderingn they cannot solve more complex or ambiguous tasks such as text generation or question answering. To capture the meaning of a text sequence, we'll use a neural network architecture called **recurrent neural network**, or RNN. In RNN, we pass our sentence through the network one token at a time, and the network produces some state, which we then pass to the network agin with the next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8fb44b2d-6824-486e-83b3-9245996e9963",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "embed_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e8fa3d-4561-425a-8bdb-89c75b39668a",
   "metadata": {},
   "source": [
    "### Simple RNN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f37c33-5cba-4d85-b6aa-fbd467b4b9b1",
   "metadata": {},
   "source": [
    "In this architecture, each recurrent unit is a simple linear network, which takes in an input vector and state vector and produces a new state vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5177062d-3d21-45b4-9035-7df72c5773b3",
   "metadata": {},
   "source": [
    "> In cases where the dimensionality isn't so high, it might make sense to pass one-hot encoded tokens directly into the RNN cell  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "73019b7c-f2c4-4dcb-ab24-69cc44ce860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization_13 (TextV (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, None, 64)          1280000   \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 16)                1296      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 1,281,364\n",
      "Trainable params: 1,281,364\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "\n",
    "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    input_shape=(1,)\n",
    ")\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    vectorizer,\n",
    "    keras.layers.Embedding(vocab_size, embed_size),\n",
    "    keras.layers.SimpleRNN(16),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8267b9b-0bdc-4c67-84e5-c3d085d7487e",
   "metadata": {},
   "source": [
    "RNNs in general are quite difficult to train, because once the RNN cells are unrolled along sequence length, the resulting number of layers involved in backpropagation is quite large. Thus we need to select a smaller learning rate, and train the network on a larger dataset to produce good results   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "258259a7-7db5-4684-bcb9-4e05577b211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(x):\n",
    "    return x['title']\n",
    "\n",
    "def tupelize_title(x):\n",
    "    return (extract_title(x), x['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "211dc15f-d80c-40d9-a5bd-fecd635845fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "7500/7500 [==============================] - 715s 95ms/step - loss: 0.6060 - acc: 0.7782 - val_loss: 0.5518 - val_acc: 0.7983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x216c7d74880>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training vectorizer')\n",
    "vectorizer.adapt(ds_train.take(2000).map(extract_title))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize_title).batch(batch_size), \n",
    "          validation_data=ds_test.map(tupelize_title).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a56230-afb9-440e-9ca7-7918f041578e",
   "metadata": {},
   "source": [
    "The `TextVectorization` layer will automatically pad sequences of variable length in a minibatch with pad tokens. Those tokens also take part in training, and they can complicate the convergence of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a6b4e7-8074-419a-b0f6-d3fe90daabf1",
   "metadata": {},
   "source": [
    "To minimize the amount of padding, there are several approaches :\n",
    "- We can reorder the dataset by sequence length and group all sequences by size. This can be done with `tf.data.experimental.bucket_by_sequence_length` function\n",
    "- We can also use masking. In Keras, some layers support additional input that shows which tokens should be taken into account when training. To incorporate masking in our model, we can either include a separate Masking layer, or we can specify the `mask_zero=True` parameter of our Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "37acf870-21da-4f7d-b155-fae30a11e15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 38452s 5s/step - loss: 0.6114 - acc: 0.7840 - val_loss: 0.7900 - val_acc: 0.7197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x216c81814f0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer,\n",
    "    keras.layers.Embedding(vocab_size, embed_size, mask_zero=True),\n",
    "    keras.layers.SimpleRNN(16),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "             validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40298dea-df62-4c9a-a4c8-b4f38bcdbe93",
   "metadata": {},
   "source": [
    "### LSTM: Long Short-Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01288a9-f32d-4a76-a180-20fa16951e31",
   "metadata": {},
   "source": [
    "One of the main problem of RNNs is vanishing gradients. RNNs can be pretty long, and may have a hard time propagating the gradients all the way back to the first layer of the network during backpropagation. When this happens, the network can't learn relationships between distant tokens. One way to avoid this problem is to introduce explicit state management by using gates. The two most common arhitectures that introduce gates are **LSTM** and **Gated Relay Unit** (GRU) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f45962-9580-491f-bb4f-ccf08feaaed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.layers.Sequential([\n",
    "    vectorizer,\n",
    "    keras.layers.Embeddingg(vocab_size, embed_size),\n",
    "    keras.layers.LSTM(8),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(8),\n",
    "             validation_data=ds_test.map(tupelize).batch(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09d651-29d3-4f9c-9191-cdbfcaf66128",
   "metadata": {},
   "source": [
    "### Bidirectional and multilayer RNNs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921cbd48-142e-4590-8f59-3f359d35e2fc",
   "metadata": {},
   "source": [
    "In our examples so far, the recurrent networks operate from the begining of a sequence until the end. For scenarios which require random access of the input sequence, it makes more sense to run the recurrent computation in both directions. RNNs that allow computations in both directions are called bidirectional RNNs, and they can be created by wrapping the recurrent layer with a special `Bidirectional` layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36bdcf7-7ab0-4bd8-ae8f-eaa503a73024",
   "metadata": {},
   "source": [
    "> The Bidirectional layer makes two copies of the layer within it, and sets the go_backwards property of one of those copies to True, making it go in the opposite direction along the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d001f64-79d5-4a47-bce5-d8eaf6bb0539",
   "metadata": {},
   "source": [
    "Recurrent networks, unidirectional or bidirectional, capture patterns within a sequence, and store them into state vectors or return them as output. As with convolutional networks, we can build another recurrent layer following the first one to capture higher level patterns, built from lower level patterns extracted by the first layer. This leads us to the notion of a **multi-layer RNN**, which consists of two or more recurrent networks, where the output of the previous layer is passed to the next layer as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16265c27-4618-45a0-a7c0-ce912e31a892",
   "metadata": {},
   "source": [
    "Keras makes constructing these networks an easy task, because you just need to add more recurrent layers to the model. For all layers except the last one, we need to specify `return_sequences=True` parameter, because we need the layer to return all intermediate states, and not just the final state of the recurrent computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aa26f1-ebe7-4d34-af9c-950ad8e051e7",
   "metadata": {},
   "source": [
    "Let's build a two-layer bidirectional LSTM for our classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9025ae3-bc89-48f0-9112-9d30c8cf0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorizer,\n",
    "    keras.layers.Embedding(vocab_size, 128, mask_zero=True),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True)),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
    "                validation_data=ds_test.map(tupelize).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713a6736-f523-410f-9b17-cd919456e0a6",
   "metadata": {},
   "source": [
    "# Generate texts with recurrent networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2663a224-18db-484e-a6f6-fc96d589b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test = tfds.load('ag_news_subset').values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5048085-e48e-456c-9fee-1846506195c8",
   "metadata": {},
   "source": [
    "## Building character vocabulary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb729027-e0c5-422a-bb31-5397d02f1516",
   "metadata": {},
   "source": [
    "To build a character-level generative network, we need to split the text into individual characters instead of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a996e3e-4c57-4b2c-840f-79da4e3095a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True,lower=False)\n",
    "tokenizer.fit_on_texts([x['title'].numpy().decode('utf-8') for x in ds_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00dc2f80-daa9-48b1-9f5c-fa059d884fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denote end-of-sequence with the token <eos>\n",
    "eos_token = len(tokenizer.word_index)+1\n",
    "tokenizer.word_index['<eos>'] = eos_token\n",
    "\n",
    "vocab_size = eos_token + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e2b3cb3-645d-4e5e-aa12-d9f544ff4bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[48, 2, 10, 10, 5, 44, 1, 25, 5, 8, 10, 13, 78]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['Hello, world!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa19dbf2-1a85-4ade-b638-022e7f6f485b",
   "metadata": {},
   "source": [
    "## Training a generative RNN to generate titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd06e66-a62d-419b-b817-6d0f85b077fc",
   "metadata": {},
   "source": [
    "To generate titles using an RNN, we will take one title as input, and for each input character in that title, we will train the network to generate the next character as output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e128beb3-d208-4460-9821-b5a91c12c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_batch(x):\n",
    "    # Extract the actual text from the string tensor\n",
    "    x = [t.numpy().decode('utf-8') for t in x]\n",
    "    # Convert the list of strings into a list of integer tensors\n",
    "    z = tokenizer.texts_to_sequences(x)\n",
    "    # Pad those tensors to their maximum length\n",
    "    z = tf.keras.preprocessing.sequence.pad_sequences(z)\n",
    "    return tf.one_hot(z, vocab_size), tf.one_hot(tf.concat([z[:,1:], tf.constant(eos_token, shape=(len(z),1))], axis=1), vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a62fcbc9-ff3e-47af-9015-e03d230715ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_batch_fn(x):\n",
    "    x = x['title']\n",
    "    a, b = tf.py_function(title_batch, inp=[x], Tout=(tf.float32, tf.float32))\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d2e3bc6-a181-4757-ae49-b39148ce0fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_1 (Masking)          (None, None, 84)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 128)         109056    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 84)          10836     \n",
      "=================================================================\n",
      "Total params: 119,892\n",
      "Trainable params: 119,892\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15000/15000 [==============================] - 945s 62ms/step - loss: 1.5454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x113b9ec59d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Masking(input_shape=(None, vocab_size)),\n",
    "    keras.layers.LSTM(128, return_sequences=True),\n",
    "    keras.layers.Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy')\n",
    "\n",
    "model.fit(ds_train.batch(8).map(title_batch_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55bb67a-b788-4a4d-864b-92aae27c7ed8",
   "metadata": {},
   "source": [
    "## Generating output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fba128-a084-4f94-819b-66da987805f8",
   "metadata": {},
   "source": [
    "We need to decode text represented by a sequence of token numbers. We could use the `tokenizer.sequences_to_texts` function, but ut does not work well with character-level tokenization. Therefore, we will take a dictionary of tokens from the tokenizer (called `word_index`), build a reverse map, and write our own decoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "494e4d6e-6766-4078-b4dc-2d2192f3708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_map = {val: key for key, val in tokenizer.word_index.items()}\n",
    "\n",
    "def decode(x):\n",
    "    return ''.join([reverse_map.get(t,'') for t in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbaae1f-7ed5-4721-9bcf-8fddb010fcd8",
   "metadata": {},
   "source": [
    "Now, let's do the generation. We first encode a string passed as parameter into a sequence, and then on each step we call our network to infer the next character"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b12145-6fe8-45b7-a3dd-7c69fef066eb",
   "metadata": {},
   "source": [
    "The output of the network is a vector of `vocab_size` elements representing probabilities of each token, and we can find the most probably token number by using `argmax`. We then append this character to the generated list of tokens, and proceed with the generation. This proces of generating one character is repeated `size` times to generate the required number of characters, and we terminate early if `eos_token` is encountered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcd5323b-eac3-4ba7-ab70-da07fd1c92f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today #39;streat to return to chief contron in the to return to set to set to set to &lt;b&gt;...&lt;/b&gt'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(model, size=100, start='Today '):\n",
    "    inp = tokenizer.texts_to_sequences([start])[0]\n",
    "    chars = inp\n",
    "    for i in range(size):\n",
    "        out = model(tf.expand_dims(tf.one_hot(inp, vocab_size), 0))[0][-1]\n",
    "        nc = tf.argmax(out)\n",
    "        if nc == eos_token:\n",
    "            break\n",
    "        chars.append(nc.numpy())\n",
    "        inp = inp+[nc]\n",
    "    return decode(chars)\n",
    "\n",
    "generate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9abf989-9981-4f5e-8138-8d8a8e53ad24",
   "metadata": {},
   "source": [
    "## Sampling output during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de6aedf-96dd-487c-82cc-ea6bd6edb288",
   "metadata": {},
   "source": [
    "The only we can see that our model is getting better is by sampling generated strings during training. We use callbacks for that, which are functions that we can pass to the `fit` function, and that will be called periodically during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d730468-dedc-496d-bec8-4770019bc2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "15000/15000 [==============================] - 969s 65ms/step - loss: 1.2761\n",
      "Today #39;barght #39; for the strike to be of the consumer service\n",
      "Epoch 2/3\n",
      "15000/15000 [==============================] - 703s 47ms/step - loss: 1.2093\n",
      "Today #39;ease #39; control of the start to be search control\n",
      "Epoch 3/3\n",
      "15000/15000 [==============================] - 823s 55ms/step - loss: 1.1781\n",
      "Today #39;early #39; and start to be security #39;\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x113b9cb38b0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_callback = keras.callbacks.LambdaCallback(\n",
    "    on_epoch_end = lambda batch, logs: print(generate(model))\n",
    ")\n",
    "\n",
    "model.fit(ds_train.batch(8).map(title_batch_fn), callbacks=[sampling_callback], epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5c6771-e90c-4f52-9005-e9146f37f66d",
   "metadata": {},
   "source": [
    "## Soft text generation and temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93803b8-5a26-4073-a7fa-c994ebbb80c3",
   "metadata": {},
   "source": [
    "In the `generate` function, we took the character with the highest probability as the next character in the generated text. This resulted in text that cycles between the same character sequences again and again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c534e59-f3dd-4b83-98a6-d06de0146223",
   "metadata": {},
   "source": [
    "However, if wee look at the probability distribution for the next character, it may be that there are several high probabilities that are pretty similar. For example, when looking for the next character in the sequence 'play', it's similar likely that it's either space or e (as in the word player)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4359f611-08a1-45c0-87bf-727deddfb010",
   "metadata": {},
   "source": [
    "Therefore, it's not always the best choice to select the character with the absolute highest probability, choosing the second or third highest might still lead to meaningful text, and may avoid cycling through charcter sequences. Therefore, a better strategy is to sample characters from the probability distribution given by the network output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa9729d-7c0e-4423-92f2-1dc8e0847d9a",
   "metadata": {},
   "source": [
    "This sampling can be done using the `np.multinominal` function which implements a multinomial distribution. A function that implements this soft text generation is defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2a6b76b-3286-4511-be9a-e44345772a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Temperature = 0.3\n",
      "Today PC Partners to State Start Cardinals (AP)\n",
      "On Sunday #3;;Well Start of Windows Madrid Street (AP)\n",
      "Moscow, S2 Wanted #39; Sale Against After With Market (AP)\n",
      "President Symon Stratege Series for Take (AP)\n",
      "Little red riding hood t candidate to be for its a security at China\n",
      "\n",
      "--- Temperature = 0.8\n",
      "Today RC Well Step Not Nekie #39; Coach Pensions Out Offensive (AP)\n",
      "On Sunday #39;athens in holiday for N #39;s castats #39;\n",
      "Moscow, RUS to retract shows death confidents in China classive &lt;b&gt;...&lt;/b&gt;\n",
      "President Arman Net Schrouble #39; Bush Like Insilitics\n",
      "Little red riding hood sin to cut at Phote fake debt change &lt;b&gt;...&lt;/b&gt;\n",
      "\n",
      "--- Temperature = 1.0\n",
      "Today onst overtown in't Sen Depution repeas - end scauds to Venney cleries (AFP)\n",
      "On Sunday pacelitiar remerits (UCliac)\n",
      "Moscow, HJPUNAS Hand Fieldreas Munch Web Iraq Summary\n",
      "President wreken #39;s latest on &lt;b&gt;...&lt;/b&gt;\n",
      "Little red riding hood on Preparsaving Fording firel condo\n",
      "\n",
      "--- Temperature = 1.3\n",
      "Today DCuncide, Me, USa for Kny stejuration your holapmes (Canadian Press)\n",
      "On Sunday brbelfury name-65:-sknea bakes at Yukon\n",
      "Moscow, FO Responiet to North A4 economy &lt;b&gt;...&lt;/b&gt;\n",
      "President #up;sevani, Rivary, refore risk,s -drabamagam\n",
      "Little red riding hood o on Zimbab\n",
      "\n",
      "--- Temperature = 1.8\n",
      "Today J Cans clway.croim ente: 3 EUT0.ISPEA5Wh N impal, movies'\n",
      "On Sunday sunled\n",
      "Moscow, hocuse) myntes\n",
      "President Eg06s GOPt datapoldnesseshway optionss\n",
      "Little red riding hood : H: prepharoys -: Youdgo, \\$P Cores  #39;Jorm'ah A Bring 7.im'Pr SETevan Fileria (Reuters)\n"
     ]
    }
   ],
   "source": [
    "def generate_soft(model,size=100,start='Today ',temperature=1.0): \n",
    "    '''\n",
    "        Temperature parameter indicates how strongly we should choose higher probability characters over\n",
    "        lower probability ones. \n",
    "        if temp is close to 0, we choose the highest probability character, and when it approaches infinity then\n",
    "        all probabilities become equal, and we randomly select the next character.\n",
    "    '''\n",
    "    inp = tokenizer.texts_to_sequences([start])[0]\n",
    "    chars = inp\n",
    "    for i in range(size):\n",
    "        out = model(tf.expand_dims(tf.one_hot(inp,vocab_size),0))[0][-1]\n",
    "        probs = tf.exp(tf.math.log(out)/temperature).numpy().astype(np.float64)\n",
    "        probs = probs/np.sum(probs)\n",
    "        nc = np.argmax(np.random.multinomial(1,probs,1))\n",
    "        if nc==eos_token:\n",
    "            break\n",
    "        chars.append(nc)\n",
    "        inp = inp+[nc]\n",
    "    return decode(chars)\n",
    "\n",
    "words = ['Today ','On Sunday ','Moscow, ','President ','Little red riding hood ']\n",
    "    \n",
    "for i in [0.3,0.8,1.0,1.3,1.8]:\n",
    "    print(f\"\\n--- Temperature = {i}\")\n",
    "    for j in range(5):\n",
    "        print(generate_soft(model,size=300,start=words[j],temperature=i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
